{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "061cbe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95be59e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1. Schema & Parser\n",
    "# =========================\n",
    "class ReviewAnalysis(BaseModel):\n",
    "    predicted_stars: int = Field(description=\"The star rating from 1 to 5\")\n",
    "    explanation: str = Field(description=\"Detailed reasoning for the score\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=ReviewAnalysis)\n",
    "\n",
    "# We escape the instructions so LangChain doesn't think the JSON schema is a variable\n",
    "# .replace(\"{\", \"{{\").replace(\"}\", \"}}\") is the trick here.\n",
    "format_instructions = parser.get_format_instructions().replace(\"{\", \"{{\").replace(\"}\", \"}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1522d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2. Data Preparation\n",
    "# =========================\n",
    "try:\n",
    "    df = pd.read_csv(\"yelp.csv\")\n",
    "    test_df = df.sample(n=5, random_state=42).reset_index(drop=True)\n",
    "except Exception:\n",
    "    # Synthetic data for immediate testing if CSV isn't present\n",
    "    print(\"CSV file not found!!!.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aef0cc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3. Initialize LLM\n",
    "# =========================\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a5bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4. Detailed & Proper Strategies \n",
    "# =========================\n",
    "\n",
    "# 1. Few-Shot (Boundary Focused)\n",
    "# We choose examples that specifically target the \"4 vs 5\" and \"1 vs 2\" boundaries.\n",
    "prompt_few_shot = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", f\"You are a Yelp calibration expert. Your goal is to distinguish between 'Good' and 'Exceptional'. {format_instructions}\"),\n",
    "    (\"human\", \"Review: Great food, we liked the pasta. Service was fine.\"),\n",
    "    (\"ai\", '{{\"predicted_stars\": 4, \"explanation\": \"A positive review without superlatives or mentions of going above-and-beyond is a 4, not a 5.\"}}'),\n",
    "    (\"human\", \"Review: This is the best meal I have ever had! Absolutely flawless service.\"),\n",
    "    (\"ai\", '{{\"predicted_stars\": 5, \"explanation\": \"Uses extreme superlatives and indicates a perfect experience.\"}}'),\n",
    "    (\"human\", \"Review: {text}\")\n",
    "])\n",
    "\n",
    "# 2. Chain-of-Thought (The \"Deductive\" Approach)\n",
    "# We force the model to look for \"Negatives\" first to counteract the 65% positivity bias.\n",
    "prompt_cot = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", f\"You are a critical reviewer. Don't be afraid to give lower scores if they are earned. {format_instructions}\"),\n",
    "    (\"human\", \"\"\"Review: {text}\n",
    "\n",
    "Analyze the review following this logic:\n",
    "Step 1: Identify any specific 'Points of Friction' (slowness, price, noise, cold food).\n",
    "Step 2: If there are ANY points of friction, the score cannot be a 5.\n",
    "Step 3: If the experience was 'fine' but lacked excitement, it is a 3 or 4.\n",
    "Step 4: Only assign a 5 if the reviewer expresses genuine delight with NO complaints.\n",
    "\n",
    "Provide your reasoning in the 'explanation' field.\"\"\")\n",
    "])\n",
    "\n",
    "# 3. Discriminative Emotion (Intensity Scaling)\n",
    "prompt_emotion = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", f\"You are an expert in sentiment nuance. {format_instructions}\"),\n",
    "    (\"human\", \"\"\"Review: {text}\n",
    "\n",
    "Compare the intensity of the sentiment:\n",
    "- 4 Stars (Satisfied): Words like 'good', 'nice', 'standard', 'will come back'.\n",
    "- 5 Stars (Thrilled): Words like 'obsessed', 'amazing', 'incredible', 'favorite'.\n",
    "\n",
    "Assign a score based on the 'vibrancy' of the language.\"\"\")\n",
    "])\n",
    "\n",
    "strategies = {\n",
    "    \"Few-Shot\": prompt_few_shot,\n",
    "    \"Chain-of-Thought\": prompt_cot,\n",
    "    \"Emotion-Weighted\": prompt_emotion\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c6467bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5. Evaluation Function\n",
    "# =========================\n",
    "def evaluate_strategy(prompt, test_df, llm):\n",
    "    # The chain now includes the parser at the end\n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    results_list = []\n",
    "    \n",
    "    for _, row in test_df.iterrows():\n",
    "        start_time = time.time()\n",
    "        predicted_val = None\n",
    "        success = False\n",
    "        \n",
    "        try:\n",
    "            # We only pass 'text' because it's the only single-brace variable left\n",
    "            response = chain.invoke({\"text\": row[\"text\"]})\n",
    "            predicted_val = response.get(\"predicted_stars\")\n",
    "            success = True\n",
    "        except Exception as e:\n",
    "            print(f\"Error during chain execution: {e}\")\n",
    "            \n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        results_list.append({\n",
    "            \"actual\": row[\"stars\"],\n",
    "            \"predicted\": predicted_val,\n",
    "            \"json_valid\": 1 if success else 0,\n",
    "            \"latency\": latency\n",
    "        })\n",
    "\n",
    "    eval_results = pd.DataFrame(results_list)\n",
    "    \n",
    "    # Metrics\n",
    "    valid = eval_results[eval_results[\"json_valid\"] == 1]\n",
    "    exact_acc = (valid[\"actual\"] == valid[\"predicted\"]).mean() if not valid.empty else 0\n",
    "    \n",
    "    return {\n",
    "        \"json_validity_pct\": eval_results[\"json_valid\"].mean() * 100,\n",
    "        \"exact_accuracy_pct\": exact_acc * 100,\n",
    "        \"avg_latency_sec\": eval_results[\"latency\"].mean()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ab24d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6. Run\n",
    "# =========================\n",
    "final_results = []\n",
    "for name, prompt in strategies.items():\n",
    "    print(f\"Running {name}...\")\n",
    "    metrics = evaluate_strategy(prompt, test_df, llm)\n",
    "    metrics[\"strategy\"] = name\n",
    "    final_results.append(metrics)\n",
    "\n",
    "print(\"\\n=== COMPARISON TABLE ===\")\n",
    "print(pd.DataFrame(final_results)[[\"strategy\", \"json_validity_pct\", \"exact_accuracy_pct\", \"avg_latency_sec\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
